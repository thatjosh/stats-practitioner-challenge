{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "file_path = os.getenv(\"FILE_PATH\")\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNP_prices_file = f'{file_path}/S&P 500 Index.csv'\n",
    "SNP_VIX_file = f'{file_path}/S&P 500 VIX.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S&P500 Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into df\n",
    "SNP = pd.read_csv(SNP_prices_file)\n",
    "SNP.columns = [\"date\", \"price\"]\n",
    "SNP[\"date\"] = pd.to_datetime(SNP[\"date\"])\n",
    "SNP = SNP.set_index(\"date\")\n",
    "SNP = SNP.sort_index(ascending=True)\n",
    "\n",
    "# Calculating log prices\n",
    "SNP[\"log_returns\"] = np.log(SNP[\"price\"]/ SNP[\"price\"].shift(1))\n",
    "SNP = SNP[1:]\n",
    "SNP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# fig = go.Figure()\n",
    "\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=SNP[\"date\"], \n",
    "#     y=SNP[\"log_returns\"], \n",
    "#     mode='lines+markers', \n",
    "#     name=\"S&P 500 Index\"\n",
    "# ))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=\"S&P 500 Index Time Series\",\n",
    "#     xaxis_title=\"Date\",\n",
    "#     yaxis_title=\"Index Value\",\n",
    "#     xaxis=dict(rangeslider=dict(visible=True)),  # Adds a range slider for better navigation\n",
    "#     template=\"plotly_white\"  # Clean background\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start and end dates\n",
    "SNP_training_start, SNP_training_end = \"2015-01-02\", \"2022-12-31\"\n",
    "SNP_testing_start, SNP_testing_end = \"2023-01-03\", \"2023-02-01\"\n",
    "\n",
    "# Filter the DataFrame\n",
    "SNP_training_df = SNP[(SNP.index >= SNP_training_start) & (SNP.index <= SNP_training_end)]\n",
    "SNP_testing_df = SNP[(SNP.index >= SNP_testing_start) & (SNP.index <= SNP_testing_end)]\n",
    "SNP_training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "def stationarity_tests(series):\n",
    "    \"\"\"\n",
    "    Performs ADF and KPSS tests on a time series.\n",
    "\n",
    "    Parameters:\n",
    "        series (pd.Series): The time series data.\n",
    "\n",
    "    Returns:\n",
    "        None (Prints test results)\n",
    "    \"\"\"\n",
    "    # Augmented Dickey-Fuller Test (ADF)\n",
    "    adf_result = adfuller(series, autolag='AIC')\n",
    "    print(\"Augmented Dickey-Fuller Test Results:\")\n",
    "    print(f\"ADF Statistic: {adf_result[0]:.4f}\")\n",
    "    print(f\"p-value: {adf_result[1]:.4f}\")\n",
    "    print(\"Critical Values:\")\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "    print(\"➡ Stationary\" if adf_result[1] < 0.05 else \"➡ Non-Stationary\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    # KPSS Test\n",
    "    kpss_result = kpss(series, regression='c', nlags=\"auto\")  # 'c' = constant (level stationarity)\n",
    "    print(\"KPSS Test Results:\")\n",
    "    print(f\"KPSS Statistic: {kpss_result[0]:.4f}\")\n",
    "    print(f\"p-value: {kpss_result[1]:.4f}\")\n",
    "    print(\"Critical Values:\")\n",
    "    for key, value in kpss_result[3].items():\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "    print(\"➡ Non-Stationary\" if kpss_result[1] < 0.05 else \"➡ Stationary\")\n",
    "\n",
    "# Run the tests on S&P 500 log returns\n",
    "stationarity_tests(SNP[\"log_returns\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hurst import compute_Hc\n",
    "\n",
    "SNP_hurst_est, c, data = compute_Hc(SNP_training_df[\"log_returns\"], kind='change', simplified=True)\n",
    "print(f\"Estimated Hurst Exponent: {SNP_hurst_est:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stochastic.processes.continuous import FractionalBrownianMotion\n",
    "import numpy as np\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def simulate_fbm_prices(train_data, test_data, H):\n",
    "    \"\"\"Simulate future stock prices using a geometric fractional Brownian motion model.\"\"\"    \n",
    "    # Calculate drift (μ) and volatility (σ) from training log returns\n",
    "    mu = train_data[\"log_returns\"].mean()\n",
    "    sigma = train_data[\"log_returns\"].std()\n",
    "\n",
    "    # Define Fractional Brownian Motion (fBM) model\n",
    "    n_days = len(test_data)\n",
    "    fbm = FractionalBrownianMotion(hurst=H, t=n_days - 1)\n",
    "    fbm_values = fbm.sample(n=n_days - 1)\n",
    "    \n",
    "    # Use the last known price from training data as the starting point\n",
    "    S0 = train_data['price'].iloc[-1]\n",
    "    simulated_prices = S0 * np.exp(mu * np.arange(n_days) + sigma * fbm_values)\n",
    "    \n",
    "    return simulated_prices\n",
    "\n",
    "def plot_actual_vs_simulated(test_data, simulated_prices, H):\n",
    "    \"\"\"Create an interactive Plotly plot comparing the actual prices with simulated prices.\"\"\"\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_data.index,\n",
    "        y=test_data[\"price\"],\n",
    "        mode='lines',\n",
    "        name='Actual Price',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_data.index,\n",
    "        y=simulated_prices,\n",
    "        mode='lines',\n",
    "        name='Forecasted Price (fBM)',\n",
    "        line=dict(color='red', dash='dash')\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=f\"Stock Price Forecast using fBM (H={H})\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Stock Price\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "simulated_prices = simulate_fbm_prices(SNP_training_df, SNP_testing_df, SNP_hurst_est)\n",
    "plot_actual_vs_simulated(SNP_testing_df, simulated_prices, SNP_hurst_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def montecarlo_simulation_and_plot(train_data, test_data, H, num_simulations=100):\n",
    "    \"\"\"Perform a Monte Carlo simulation by generating multiple fBM price paths and plot them \n",
    "    alongside the actual stock prices.\"\"\"\n",
    "    \n",
    "    # Calculate drift (μ) and volatility (σ) from training log returns\n",
    "    mu = train_data[\"log_returns\"].mean()\n",
    "    sigma = train_data[\"log_returns\"].std()\n",
    "    \n",
    "    n_days = len(test_data)\n",
    "    time_points = np.arange(n_days)\n",
    "    S0 = train_data['price'].iloc[-1]\n",
    "    \n",
    "    simulation_paths = []\n",
    "    \n",
    "    for i in range(num_simulations):\n",
    "        # For each simulation, sample a new fBM path\n",
    "        fbm = FractionalBrownianMotion(hurst=H, t=n_days - 1)\n",
    "        fbm_values = fbm.sample(n=n_days - 2)\n",
    "        fbm_values = np.insert(fbm_values, 0, 0)  # align with the time vector\n",
    "        sim_prices = S0 * np.exp(mu * time_points + sigma * fbm_values)\n",
    "        simulation_paths.append(sim_prices)\n",
    "    \n",
    "    # Create a Plotly figure for the Monte Carlo simulation\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add each simulated path (using low opacity to avoid clutter)\n",
    "    for i, sim_prices in enumerate(simulation_paths):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=test_data.index,\n",
    "            y=sim_prices,\n",
    "            mode='lines',\n",
    "            name=f'Simulation {i+1}',\n",
    "            line=dict(color='rgba(255, 0, 0, 0.1)'),  # Red with transparency\n",
    "            showlegend=False  # Hide individual simulation legends\n",
    "        ))\n",
    "    \n",
    "    # Overlay the actual prices\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_data.index,\n",
    "        y=test_data[\"price\"],\n",
    "        mode='lines',\n",
    "        name='Actual Price',\n",
    "        line=dict(color='blue', width=2)\n",
    "    ))\n",
    "    \n",
    "    # Update plot layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Monte Carlo Simulation of Stock Prices using fBM (H={H}) - {num_simulations} Paths\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Stock Price\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "montecarlo_simulation_and_plot(SNP_training_df, SNP_testing_df, SNP_hurst_est, num_simulations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rolling_hurst(df, window, col='log_returns', lag=0):\n",
    "    hurst_values = []\n",
    "    times = []\n",
    "    \n",
    "    # Loop through the DataFrame using the rolling window\n",
    "    for i in range(window - 1, len(df), lag+1):\n",
    "\n",
    "        # Extract the window slice from the series\n",
    "        window_series = df[col].iloc[i - window + 1 : i + 1: lag+1]\n",
    "        \n",
    "        # Compute the Hurst exponent using the 'change' method and simplified calculation\n",
    "        h, c, data = compute_Hc(window_series, kind='change', simplified=True)\n",
    "        \n",
    "        hurst_values.append(h)\n",
    "        times.append(df.index[i])\n",
    "    \n",
    "    # Create and return a new DataFrame with the computed Hurst exponents\n",
    "    result_df = pd.DataFrame({'hurst': hurst_values}, index=times)\n",
    "    return result_df\n",
    "\n",
    "hurst_window150_lag0 = compute_rolling_hurst(SNP, window=150, col='log_returns', lag=0)\n",
    "hurst_window350_lag0 = compute_rolling_hurst(SNP, window=350, col='log_returns', lag=0)\n",
    "hurst_window500_lag0 = compute_rolling_hurst(SNP, window=500, col='log_returns', lag=0)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=hurst_window150_lag0.index,\n",
    "    y=hurst_window150_lag0[\"hurst\"],\n",
    "    mode='lines',\n",
    "    name='150 days rolling window',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=hurst_window350_lag0.index,\n",
    "    y=hurst_window350_lag0[\"hurst\"],\n",
    "    mode='lines',\n",
    "    name='350 days rolling window',\n",
    "    line=dict(color='red')\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=hurst_window500_lag0.index,\n",
    "    y=hurst_window500_lag0[\"hurst\"],\n",
    "    mode='lines',\n",
    "    name='500 days rolling window',\n",
    "    line=dict(color='green')\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=f\"Hurst exponent for SNP\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Hurst Value\",\n",
    "    template=\"plotly_white\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hit_ratio(df, hurst_col='hurst', return_col='log_returns', threshold=0.5):\n",
    "    \"\"\"Compute the hit ratio for a trading strategy based on the Hurst exponent.\"\"\"\n",
    "    df = df.dropna().copy()  # Ensure no NaNs\n",
    "\n",
    "    # Generate trading signals based on Hurst exponent\n",
    "    df['signal'] = np.where(df[hurst_col] > threshold, 1, -1)  # 1 for trending, -1 for mean-reverting\n",
    "\n",
    "    # Shift returns forward to compare with the signal\n",
    "    df['future_return'] = df[return_col].shift(-1)  # Next-period return\n",
    "\n",
    "    # Correct predictions: signal * future return > 0 means correct prediction\n",
    "    df['correct'] = np.sign(df['signal'] * df['future_return']) == 1\n",
    "\n",
    "    # Compute hit ratio\n",
    "    hit_ratio = df['correct'].mean()\n",
    "    \n",
    "    return hit_ratio\n",
    "\n",
    "hit_150_0_df = SNP.join(hurst_window150_lag0, how='inner')\n",
    "hit_350_0_df = SNP.join(hurst_window350_lag0, how='inner')\n",
    "hit_500_0_df = SNP.join(hurst_window500_lag0, how='inner')\n",
    "\n",
    "hit_150_0 = compute_hit_ratio(hit_150_0_df)\n",
    "hit_350_0 = compute_hit_ratio(hit_350_0_df)\n",
    "hit_500_0 = compute_hit_ratio(hit_500_0_df)\n",
    "\n",
    "print(f\"Hit Ratios:\\n\"\n",
    "      f\"- 150-day window (0 lag): {hit_150_0:.2%}\\n\"\n",
    "      f\"- 350-day window (0 lag): {hit_350_0:.2%}\\n\"\n",
    "      f\"- 500-day window (0 lag): {hit_500_0:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using different lags\n",
    "hurst_window150_lag2 = compute_rolling_hurst(SNP, window=150, col='log_returns', lag=0)\n",
    "hurst_window350_lag2 = compute_rolling_hurst(SNP, window=350, col='log_returns', lag=2)\n",
    "hurst_window500_lag2 = compute_rolling_hurst(SNP, window=500, col='log_returns', lag=4)\n",
    "\n",
    "hit_150_2_df = SNP.join(hurst_window150_lag2, how='inner')\n",
    "hit_350_2_df = SNP.join(hurst_window350_lag2, how='inner')\n",
    "hit_500_2_df = SNP.join(hurst_window500_lag2, how='inner')\n",
    "\n",
    "hit_150_2 = compute_hit_ratio(hit_150_2_df)\n",
    "hit_350_2 = compute_hit_ratio(hit_350_2_df)\n",
    "hit_500_2 = compute_hit_ratio(hit_500_2_df)\n",
    "\n",
    "print(f\"Hit Ratios:\\n\"\n",
    "      f\"- 150-day window: {hit_150_2:.2%}\\n\"\n",
    "      f\"- 350-day window: {hit_350_2:.2%}\\n\"\n",
    "      f\"- 500-day window: {hit_500_2:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run fBM Model on data post-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "snp_obj = utils.SnpDf(SNP)\n",
    "result_df = utils.apply_rolling_predictions_from_start(snp_obj, '2022-01-01', 150)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = result_df.join(SNP, how='inner')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.compute_rmse(merged_df, 'predicted', 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=merged_df.index,\n",
    "    y=merged_df[\"predicted\"],\n",
    "    mode='lines',\n",
    "    name='Predicted',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=merged_df.index,\n",
    "    y=merged_df[\"price\"],\n",
    "    mode='lines',\n",
    "    name='Actual',\n",
    "    line=dict(color='red')\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=f\"Predicted vs Actual Volatility\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Volatility\",\n",
    "    template=\"plotly_white\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from scipy import stats\n",
    "\n",
    "# Example DataFrame: df should already have a time series index and the columns 'observed' and 'predicted'\n",
    "# df = pd.read_csv('your_data.csv', index_col='date', parse_dates=True)\n",
    "\n",
    "# 1. Compute the residuals\n",
    "df = merged_df \n",
    "df['residual'] = df['price'] - df['predicted']\n",
    "\n",
    "# 2. Plot the residuals over time\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df.index, df['residual'], label='Residuals')\n",
    "plt.title(\"Residuals Over Time\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Large spikes, indicating many outliers \n",
    "- Residuals oscillate around zero (good)\n",
    "- No obvious trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Plot the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF)\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "plot_acf(df['residual'], lags=20, ax=ax[0])\n",
    "plot_pacf(df['residual'], lags=20, ax=ax[1])\n",
    "ax[0].set_title(\"ACF of Residuals\")\n",
    "ax[1].set_title(\"PACF of Residuals\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- No signification autocorrelation (since no spikes), captured temporal dependencies well\n",
    "- Good fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Q-Q Plot for normality check of the residuals\n",
    "plt.figure(figsize=(6, 6))\n",
    "stats.probplot(df['residual'], dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot of Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Observed heavy tails, indicating that there are outliers\n",
    "- Non-normal distribution, (probably t distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Histogram of the residuals\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df['residual'], kde=True)\n",
    "plt.title(\"Histogram of Residuals\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Slightly negative, a bit of overpredicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Perform a normality test (Shapiro-Wilk Test)\n",
    "stat, p_value = stats.shapiro(df['residual'])\n",
    "print(f'Shapiro-Wilk Test statistic: {stat:.4f}, p-value: {p_value:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "-  A very small p-value means we reject the null hypothesis that states that residuals are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts from plots:\n",
    "- Standard fBM assumes increments follow a normal distribution, but clearly this isn't the case here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Volatility Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_obj = utils.SnpDf(SNP)\n",
    "high_vol_df = utils.apply_rolling_predictions_from_start(snp_obj, '2020-01-01', 150)\n",
    "high_vol_df = high_vol_df[(high_vol_df.index >= '2020-01-01') & (high_vol_df.index <= '2022-01-01')]\n",
    "high_vol_df = high_vol_df.join(SNP, how='inner')\n",
    "utils.compute_rmse(high_vol_df, 'predicted', 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Volatility Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_obj = utils.SnpDf(SNP)\n",
    "low_vol_df = utils.apply_rolling_predictions_from_start(snp_obj, '2018-01-01', 150)\n",
    "low_vol_df = low_vol_df[(low_vol_df.index >= '2018-01-01') & (low_vol_df.index <= '2020-01-01')]\n",
    "low_vol_df = low_vol_df.join(SNP, how='inner')\n",
    "utils.compute_rmse(low_vol_df, 'predicted', 'price')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
